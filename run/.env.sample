#### REMOTE SERVERS
# Worker list (NodeID 0 should be the last in the list)
# 16 workers
SERVERS=("ai-cpu-016" "ai-cpu-015" "ai-cpu-014" "ai-cpu-013" "ai-cpu-012" "ai-cpu-011" "ai-cpu-010" "ai-cpu-009" "ai-cpu-008" "ai-cpu-007" "ai-cpu-006" "ai-cpu-005" "ai-cpu-004" "ai-cpu-003" "ai-cpu-002" "ai-cpu-001")
# 8 workers
#SERVERS=("ai-cpu-008" "ai-cpu-007" "ai-cpu-006" "ai-cpu-005" "ai-cpu-004" "ai-cpu-003" "ai-cpu-002" "ai-cpu-001")
# 4 workers
#SERVERS=("ai-cpu-004" "ai-cpu-003" "ai-cpu-002" "ai-cpu-001")
# 2 workers
#SERVERS=("ai-cpu-002" "ai-cpu-001")         
# 1 worker
#SERVERS=("ai-cpu-001")
PASSWORD=                                              # ssh Passphrase for jumphost ssh keyfiles
NWORKERS="${#SERVERS[@]}"                              # Number of workers (VMs)

#### MODEL CHECKPOINT
REMOTE_DIR="workspace/llama"                           # Working directory in remote server
TOKENIZER_PATH="tokenizer.model"                       # Tokenizer path for LLaMa models
SOURCE_CHECKPOINT="models/1-workers/"                  # 7B source checkpoint
SHARD_PATH="models/$NWORKERS-workers/"                 # Model checkpoint/shard path

DOCKER_CONTAINER_NAME="docker_llama-dev_1"             # Container name

#### INFERENCE
MASTER_ADDR=                                           # IP address of master node. Eth1 in Azure VMs
MASTER_PORT=12345                                      # Port for TCP communication


DEVICE="cpu"                                           # "cuda" for GPU platform
TASK="text"                                            # text or chat 
PROMPT_FILE="prompts/${TASK}_completion_example.yml"   # promt file, selected based on task
BATCH=4                                                # Notes: 
                                                       #  - For text completion task, default batch size is 4. 
                                                       #  - For chat completion task, dafault batch sise is 6.

NCORES="All"                                           # Use all physical cores in the worker/VM. Valid only for device="cpu" platforms. 
DO_PROFILE=0                                           # Flag (0 disable, 1 enable) to profile inference with Pytorch profiler.
PROFILE_OUTPUT="/app/log/test"                         # Default output folder of profiler traces: log/test (/app/log/test inside container) 


#### WORKER CONFIGURATION
# Docker compose 
DOCKER_COMPOSE_VERSION="1.29.2"                       
DOCKER_COMPOSE_URL="https://github.com/docker/compose/releases/download/${DOCKER_COMPOSE_VERSION}/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose"

# ai-virtual-gpu repository URL and branch
GIT_USER=                                              # Git user
GIT_PASSWORD=                                          # Git personal access token
REPO_URL="https://$GIT_USER:$GIT_PASSWORD@github.com/wiiit-artificial-intelligence/llama.git"
REPO_BRANCH="distributed_inference"


META_URL=""                                            # Request a download link:  https://ai.meta.com/resources/models-and-libraries/llama-downloads/.
MODEL_VERSION=""                                       # (7B,13B,70B,7B-chat,13B-chat,70B-chat)
